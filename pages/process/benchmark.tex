\subsection{Sistema de evaluación del modelo}
Un buen sistema de evaluación es esencial para un proyecto de Machine Learning.
No solo es útil por el hecho de medir el rendimiento de los modelos,
sino que además sirve como un identificador claro del progreso dentro del desarrollo.
El seleccionar el modelo óptimo en fases tempranas del proceso no es tan importante,
como el hecho de saber que las iteraciones en las que gastamos tiempo y recursos están teniendo un impacto
positivo en el resultado final. Aunque físicamente es imposible conseguir una productividad
perfecta, con ayuda de una metodología sólida es posible reducir al máximo esta problemática.

\subsubsection{Selección de métrica}
El objetivo de una métrica es, mediante ponderación numérica, identificar errores en el modelo y
servir como ayuda para la toma de decisiones, cambios en el diseñe o la solución de problemas. En el caso del
FJSP, el objetivo es encontrar una acorde a la tarea que queramos abordar. En la literatura se atacan dos puntos
principalmente: \textit{minimizar el tiempo de finalización} y \textit{minimizar el tiempo de espera entre máquinas}.

\begin{itemize}
    \item \textbf{Minimizar el tiempo de espera entre máquinas:} mide el tiempo promedio que cada operación debe
          esperar antes de ser procesado en una máquina específica. Esta métrica es importante porque
          indica la eficiencia del sistema en términos de cómo se están utilizando los recursos disponibles
          y cómo se gestionan los trabajos en las colas.
    \item \textbf{Minimizar el tiempo de finalización:} se basa en el tiempo total que se tarda en completar todos
          los trabajos del sistema. Es un buen indicador porque evalúa el rendimiento global del sistema
          y la eficacia para completar el proceso en el plazo establecido.
\end{itemize}

Aunque ambas métricas son importantes, la segunda es la que vamos a utilizar como referencia en cuanto al
desempeño del modelo se refiere, ya que uno de nuestros objetivos principales es reducir los tiempos de producción y no tanto
aprovechar las máquinas lo máximo posible. Es una opción combinar ambas para identificar puntos débiles tanto del modelo
como del propio proceso que se quiere optimizar pero, por la propia idiosincrasia de estas métricas, el mejorar la puntuación
en una inevitablemente afectara negativamente a la valoración de la otra en un amplio número de casuísticas.

\subsubsection{Implementación de la métrica}
Ya hemos decidido que métrica es la indicada para evaluar nuestro modelo, pero ahora debemos
implementarla de acorde al contexto de nuestro problema. El siguiente paper utiliza una formula para
calcular el time spam que nosotros también vamos a utilizar, donde Tmax es el tiempo que tarda en
finalizar el sistema y Tmin el tiempo óptimo de finalización. Nuestro objetivo como hemos mencionado
previamente es acercarnos lo máximo posible a la combinación optima, que en este caso seria lo equivalente a
aproximarnos lo más que podamos al 0. Además, gracias a que nuestra implementación se basa en una función
matemática, podremos extrapolarla en el apartado de \textit{Benchmarking} no solo a la evaluación de nuestro
modelo, sino también a cualquier otro sistema externo que resuelva el problema.

\medskip
\equationNote{f(x) = \frac{Tmax - Tmin}{Tmin}}{}

\subsubsection{Benchmarking}
Por último, es hora de definir como vamos a realizar el proceso de benchmarking. Hemos comentado en
el apartado anterior, que no solo incluiremos el histórico de las diferentes versiones de nuestro
modelo, sino que también añadiremos otras alternativas para la resolución del problemas como reglas de
ordenación, algoritmos genéticos o métodos exactos. Para ello, contamos con un benchmark de 10 instancias
llamada Brandimarte \cite*{pbrandimarte}, que son las populares dentro de la literatura para la comparación de modelos.
Estas se pueden encontrar en el siguiente repositorio \cite*{ptal} de GitHub.\medskip

\begin{table}[ht]
    \centering
    \begin{tabular}[ht]{c|ccccc|c} 
                  & njob & nmac & nop & meq & proc & optim \\
        \hline 
        MK01      & 10  & 06    & 05 -- 07  & 3  & 01 -- 07  & 40\\
        MK02      & 10  & 06    & 05 -- 07  & 6  & 01 -- 07  & 26\\
        MK03      & 15  & 08    & 10 -- 10  & 5  & 01 -- 20  & 204\\
        MK04      & 15  & 08    & 03 -- 10  & 3  & 01 -- 10  & 60\\
        MK05      & 15  & 04    & 05 -- 10  & 2  & 05 -- 10  & 172\\
        MK06      & 10  & 15    & 15 -- 15  & 5  & 01 -- 10  & 57\\
        MK07      & 20  & 05    & 05 -- 05  & 5  & 01 -- 20  & 139\\
        MK08      & 20  & 10    & 10 -- 05  & 2  & 05 -- 20  & 523\\
        MK09      & 20  & 10    & 10 -- 15  & 5  & 05 -- 20  & 307\\
        MK10      & 20  & 15    & 10 -- 15  & 5  & 05 -- 20  & 196\\
    \end{tabular}
    \caption{Informacion del benchmark}
\end{table}

En la tabla anterior, podemos observar las diferentes características de cada instancia. A continuación,
vamos a explicar cada una de ellas:
\begin{itemize}
    \item \textbf{njob:} Número de trabajos.
    \item \textbf{nmac:} Número de máquinas.
    \item \textbf{nop:} Número mínimo y máximo de operaciones por trajo.
    \item \textbf{meq:} Número máximo de máquinas equivalentes por operación.
    \item \textbf{proc:} Número mínimo y máximo de tiempo de procesamiento por operación.
    \item \textbf{optim:} Valor óptimo para la resolución.
\end{itemize} 

Además, el paper \cite*{pbrandimarte} también incluye una tabla con los resultados de diferentes
reglas de ordenación y otra clase de algoritmos como la búsqueda tabú. Esto nos sera de gran ayuda
para comparar nuestro modelo con otros y ver si realmente estamos mejorando los tiempos en
comparación con otras alternativas, no solo en el caso de que nuestro modelo sea mejor, sino también
en el caso de que no lo sea. En este último caso, podremos analizar los resultados y ver si es posible
realizar cambios en el enfoque del problema para mejorar los resultados.